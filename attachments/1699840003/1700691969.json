{
    "lesson09_Hive.pptx": {
        "lesson_number": "lesson09",
        "lesson_name": "Hive",
        "course_resources": [
            "https://blogs.sap.com/2017/07/19/bridging-two-worlds-integration-of-sap-and-hadoop-ecosystems/",
            "https://cwiki.apache.org/confluence/display/Hive/Home",
            "https://www.linkedin.com/learning/learning-hadoop-2/understanding-hive?autoplay=true&resume=false&u=2212217",
            "https://cwiki.apache.org/confluence/display/Hive/Apache+Hive+SQL+Conformance",
            "https://www.vldb.org/pvldb/vol2/vldb09-938.pdf",
            "https://www.linkedin.com/learning/learning-hadoop-2/revisiting-wordcount-using-hive?autoplay=true&resume=false&u=2212217"
        ],
        "citations": [
            "Practical Hive: A Guide to Hadoop's Data Warehouse System, S. Shaw, A. F. Vermeulen, A. Gupta, D. Kjerrumgaard. Apress, 2016."
        ],
        "content": "Lesson 09 Hive PROG73010 Introduction to Big Data Fall 2022 Lesson Outline Hadoop Ecosystem Hive Hive Demo Hadoop Ecosystem Image from SAP article, Bridging two worlds : Integration of SAP and Hadoop Ecosystems Hadoop Ecosystem Evolution The Hadoop landscape rapidly changes Release cycles are measured in months Patches and updates are measured in weeks Active and vibrant open-source community Adoption drives innovation Hadoop Ecosystem Hive A small but critical component of the Hadoop ecosystem Critical because it is the entry point into an exceedingly complex data storage environment A link between the traditional and the new A nod by the Hadoop development community that 50 years of RDBM design and access is of value and useful and worth the effort in order to drive adoption Hive Hive strives to be Hadoop access for the masses Hadoop for all its scalability and redundancy is nothing without adoption by the users who perform analysis and insight in an organization Data is nothing if it is not useful, easily accessible, or provides immediate ROI SQL is the natural language for data and the obvious choice for general Hadoop analysis Provides ease of use, common understanding, and flexibility Hive is not 100% mapped to ANSI SQL Utilizes core parts of traditional SQL Allows business analysts to quickly adapt to and function on the Hadoop environment Hive Alternatives Other SQL on Hadoop engines exist Impala, HAWQ, and Spark SQL Each has its benefits and drawbacks, areas of strength and areas of weakness They all understand the value of providing interactive SQL capabilities on Hadoop along with the performance we expect from traditional business intelligence infrastructures Hive stands out with its widespread adoption and diverse development community represented by some of the largest IT organizations in the world Relational Database Systems Relational systems have been a valuable tool for many decades The SQL query language brought data access to the masses by abstracting away concepts such as data location and instead allowed developers to focus on how the data will be presented SQL excels as a declarative language in which you clearly specify what you want to do in simple English language syntax E.g., You can SELECT data FROM a source WHERE the value equals, or does not equal, something The developer does not have to worry about the data structure or where the data resides on disk Early Days of Hadoop Problem Early Hadoop adoption involved HDFS as the storage system and MapReduce as the compute framework Java is the language of MapReduce Early in the Hadoop adoption, if you needed to perform computation and access data in Hadoop, you had to write Java MapReduce programs Facebook realized that you could not hire enough Java developers to write the amount of MapReduce code needed to take full advantage of the quantity of data stored in HDFS Early Days of Hadoop Solution To increase adoption and ease of use, developers needed to abstract away MapReduce complexity The answer was SQL (Structured Query Language) MapReduce would stay as the compute language but would be relegated to behind-the-scenes functionality HiveQL became a language a business analyst could adopt because the syntax looked like SQL Could still take advantage of the parallel processing power of MapReduce Hive History Apache Hive was co-created by Joydeep Sen Sarma and Ashish Thusoo at Facebook To abstract the complexities of writing Java MapReduce The original whitepaper was published in 2009 The whitepaper can be found at this link Hive has developed from a simple SQL veneer over MapReduce to a fully functional interactive framework Hive vs. SQL Hive looks like SQL, but remember it is not SQL This is especially evident in processing speed SQL is an interactive processing language Hive queries still run as MapReduce jobs MapReduce is batch processing Hive SQL Conformance You can find the current level of SQL conformance in Hive at this link Hive Query Execution Originally developed to run MapReduce jobs Hive can now change its query execution engine MapReduce Spark Tez Apache Hive Confluence, https://cwiki.apache.org/confluence/display/Hive/Home Hive Components Hive is not a standalone tool Relies on various components for storing and querying data Hive is considered a client data access tool within the Hadoop ecosystem Hive Components Metastore Metastore is where Hive keeps all its metadata Table and relation information Column names, column types, etc. Implemented using tables in a relational database You can select the RMDB Derby (default) MySQL MS SQL Server Oracle Postgres Hive Components HCatalog HCatalog is a table and storage management layer Facilitates the schema-on-read Is built on top of the Hive Metastore and incorporates Hive's DDL Responsible for starting the Metastore Exposes Hive data and metadata to other technologies directly Hive Video Reiteration Understanding Hive Revisiting WordCount using Hive Hive Demo Thank you! Any questions?"
    },
    "lesson00_CourseIntroduction.pptx": {
        "lesson_number": "lesson00",
        "lesson_name": "CourseIntroduction",
        "course_resources": [
            "mailto:dallison@conestogac.on.ca",
            "https://learning.oreilly.com/"
        ],
        "citations": [],
        "content": "Course Introduction PROG73010 Introduction to Big Data Fall 2022 Agenda Contact Information Course Description Course Outcomes Units Sources Evaluation Contact Information David Allison, Ph.D., P.Eng. Professor Coordinator Email: dallison@conestogac.on.ca Email me directly to this email using your conestogac.on.ca email Do not use personal email Office: 4G26 Office Hours: Email for appointment Course Description We now live in a world with immense amounts of complex data which is subject to rapid change. Analyzing these data sets can reveal trends, patterns, and relationships which can bring insight and present new opportunities to organizations. In this course, students will explore fundamental concepts around big data problems, applications, and challenges. Through hands-on exercises, labs and interactive lectures, students will solve big data problems using industry standard best practices, programming models/languages, tools and techniques. Students will also develop an understanding of big data strategies and their importance in the success of todays organizations. Course Outcomes Explain key concepts of Big Data to address business problems associated with the collection of large data sets. Identify business motivations for the adoption of Big Data solutions in accordance with customer and industry needs. Compare Big Data systems to traditional data systems in order to select a solution that fits the needs of a project. Course Outcomes (continued) Investigate challenges to Big Data solutions in order to address issues meeting customer and industry expectations. Analyze large date sets to reveal knowledge and trends to create new business opportunities. Employ modern algorithms, tools and technologies to process large volumes of data according to industry best practices. Unit 1 Big Data Concepts Define fundamental concepts within the field of Big Data. Categorize types of data analytics by the different results they produce. Explain Business Intelligence and its role within an enterprise system. Unit 2 Big Data Characteristics Classify data as \"big data\" according to the characteristics of volume, velocity, variety, veracity and value. Differentiate data formats that are processed by Big Data solutions. Unit 3 Big Data Adoption Drivers Discuss business motivations for the adoption of Big Data solutions and technology. Examine the limitations of traditional data systems. Outline how Big Data analytic results can help organizations deliver value to customers. Examine how different information and communications technologies have accelerated the industry adoption of Big Data. Unit 4 Big Data Adoption Considerations Summarize what an organization needs in place before a Big Data solution should be implemented. Discuss different Big Data platforms and how they differ. Examine the sources of external data for businesses. Unit 5 Big Data Adoption Concerns Explain privacy, security and ethic issues surrounding the collection and use of Big Data. Discuss the importance of provenance information and the challenges maintaining this information presents. Examine technological and organizational requirements to maintain Big Data solutions. Unit 6 Big Data Business Intelligence Discuss enterprise technologies which support the transformation of data into knowledge. Explain the role of a data warehouse and now they are used to discover business intelligence. Contrast the differences between traditional business intelligence and Big Data business intelligence. Unit 7 Big Data Storage Concepts Discuss the need for and advantages from using clusters of computers for Big Data. Compare traditional file systems to distributed file systems. Examine the use of sharding and replication in producing robust Big Data storage. Unit 8 Big Data Processing Concepts Discuss the use of parallel and distributed data processing techniques to process large volumes of data. Examine industry standard data storage and processing frameworks. Explain the use of industry standard programming models within Big Data frameworks. Unit 9 Big Data Programming Establish a Big Data environment using industry standard tools and frameworks. Examine the characteristics of programming for Big Data frameworks. Contrast programming language options for creating Big Data processing functions. Create and run Big Data processing functions using different programming languages. Sources Big Data Fundamentals: Concepts, Drivers & Techniques, 1st Edition, T. Erl, W. Khattak, P. Buhler. Pearson, 2016 Available on O'Reilly Online Learning Click here for link to O'Reilly Online Learning Evaluation Six Assignments 60% Midterm 20% Final 20% Marks will be posted on eConestoga Minimum passing grade for this course is 60 (C) Academic Integrity Violations Work will be compared and checked Big Data Stream Introduction to Big Data Big Data Integration and Processing Data Analysis Tools Big Data Modelling, Visualization and Management Systems Big Data and Cloud Computing Graph Analytics for Big Data Machine Learning with Big Data Level 6 Level 7 Level 8 Level 9 Thank you! Any questions?"
    },
    "lesson10_Pig.pptx": {
        "lesson_number": "lesson10",
        "lesson_name": "Pig",
        "course_resources": [
            "https://pig.apache.org/docs/r0.17.0/basic.html#Data+Types+and+More",
            "https://dl.acm.org/doi/10.1145/1376616.1376726",
            "https://blogs.sap.com/2017/07/19/bridging-two-worlds-integration-of-sap-and-hadoop-ecosystems/",
            "https://pig.apache.org/philosophy.html"
        ],
        "citations": [
            "Programming Pig, 2nd Edition, A. Gates, D. Dai. O'Reilly Media, 2016."
        ],
        "content": "Lesson 10 Pig PROG73010 Introduction to Big Data Fall 2022 Lesson Outline Hadoop Ecosystem Pig Pig Demo Hadoop Ecosystem Image from SAP article, Bridging two worlds : Integration of SAP and Hadoop Ecosystems What is Pig? Pig provides an engine for executing data flows in parallel on Apache Hadoop Data flows expressed in custom language, Pig Latin Pig Latin includes operators for many of the traditional data operations (join, sort, filter, etc.), as well as providing the ability for users to develop their own functions for reading, processing, and writing data What is Pig? (continued) A high-level abstraction to create MapReduce jobs to run Extract, Transform, Load (ETL) processes Abstraction like we saw in Hive Use when doing ETL-like jobs Transform data Clean data Process data Pig History Began as a research project at Yahoo! Pig High-level platform for creating MapReduce jobs Pig Latin Language used to create pig scripts and jobs PiggyBank A repository for Pig functions Open sourced in 2007, became subproject of Apache Hadoop in 2008, and top-level Apache project in 2010 Motivation for Pig As described in an introductory 2008 paper: Many of the people who analyze this data are entrenched procedural programmers, who find the declarative, SQL style to be unnatural. The success of the more procedural map-reduce programming model, and its associated scalable implementations on commodity hardware, is evidence of the above. However, the map-reduce paradigm is too low-level and rigid, and leads to a great deal of custom user code that is hard to maintain, and reuse. We describe a new language called Pig Latin that we have designed to fit in a sweet spot between the declarative style of SQL, and the low-level, procedural style of map-reduce. The accompanying system, Pig, is fully implemented, and compiles Pig Latin into physical plans that are executed over Hadoop, an open-source, map-reduce implementation. Apache Pig Philosophy The Apache Pig Project has founding principles that help developers decide how the system should grow over time Pigs Eat Anything Pigs Live Anywhere Pigs are Domestic Animals Pigs Fly Pigs Eat Anything Pig can operate on data whether it has metadata or not It can operate on data that is relational, nested, or unstructured It can easily be extended to operate on data beyond files, including key/value stores, databases, etc. Pigs Live Anywhere Pig is intended to be a language for parallel data processing It is not tied to one particular parallel framework It has been implemented first on Hadoop, but is not intend that to be only on Hadoop Pigs are Domestic Animals Pig is designed to be easily controlled and modified by its users Pig allows integration of user code wherever possible Supports user defined field transformation functions, aggregates, and conditionals These functions can be written in Java or scripting languages that can compile down to Java Pig has an optimizer that rearranges some operations in Pig Latin scripts to give better performance, combines Map Reduce jobs together, etc. Can turn this optimizer off Pigs Fly Pig processes data quickly Performance improvement is prioritized Do not want to implement features in ways that weigh pig down so it can't fly Pig Latin A data flow language Allows users to describe how data from one or more inputs should be read, processed, and then stored to one or more outputs in parallel These data flows can be simple linear flows, or complex workflows that include points where multiple inputs are joined and where data is split into multiple streams to be processed by different operators Pig Latin looks different from many of the programming languages No if statements or for loops Traditional programming languages control flow, and data flow is a side effect of the program Pig Latin instead focuses on data flow Pig Latin Structure LOAD <FUNCTIONS> DUMP STORE Pig's Data Model Simple Data Types Scalar, single value int, float, double, boolean, chararray, datetime, etc. More details can be found online Pig's Data Model Complex Data Types Map chararray (key) maps an element of any Pig type (value) [name#Arthur], [name#Zaphod,age#42] Tuple Fixed-length, ordered collection of Pig data elements (Ford,20,4.2F) Bag A collection of tuples {tuple}, {tuple1, tuple2, tuple3} Relation A collection of bags A = LOAD 'student' USING PigStorage() AS (name:chararray, age:int, gpa:float); DUMP A; (Alice,18,4.0F) (Bob,19,1.2F) (Carl,20,3.2F) Data Flow vs. Query Languages Query languages focus on questions to be answered \"How many students are above the age of 25?\" SELECT fname, lname FROM students WHERE age > 25; Does not state how the question should be answered Data flow languages focus on exactly how to process data Data Flow vs. Query Languages (continued) Query languages focus on answering one question When you want a second question answered: Can write a separate query and dump data into a temporary table Or use sub-queries inside the original query Often found to be confusing Inside-out design Data flow allows for a long series of operations Data Flow vs. Query Languages Example Case A user wants to group one table on a key and then join it with a second table Recall: In SQL joins happen before grouping, so this must be expressed either as a subquery or as two queries with the results stored in a temporary table Data Flow vs. Query Languages Example Case in SQL CREATE TEMP TABLE t1 AS SELECT customer, sum(purchase) AS total_purchases FROM transactions GROUP BY customer; SELECT customer, total_purchases, zipcode FROM t1, customer_profile WHERE t1.customer = customer_profile.customer; Data Flow vs. Query Languages Example Case in Pig Latin txns = load 'transactions' as (customer, purchase); grouped = group txns by customer; total = foreach grouped generate group, SUM(txns.purchase) as tp; profile = load 'customer_profile' as (customer, zipcode); answer = join total by group, profile by customer; dump answer; Data Flow vs. Query Languages Environments SQL is designed for relational environments Data is normalized Schemas are enforced Constraints are enforced Pig is designed for Hadoop data-processing environments Data rarely normalized Schemas often unknown Data often has no constraints Pig does not require data to be placed into tables Can operate on data as soon as it is copied into HDFS Running Pig Scripts Interactive mode Uses custom shell: Grunt $ pig grunt> Batch mode Run a Pig Latin script saved in a single file from the command line $ pig filename.pig Embedded mode Run within Java Pig Demo Thank you! Any questions?"
    },
    "lesson06_BigDataStorage.pptx": {
        "lesson_number": "lesson06",
        "lesson_name": "BigDataStorage",
        "course_resources": [],
        "citations": [
            "Big Data Fundamentals: Concepts, Drivers & Techniques, 1st Edition, T. Erl, W. Khattak, P. Buhler. Pearson, 2016."
        ],
        "content": "Lesson 06 Big Data Storage Concepts PROG73010 Introduction to Big Data Fall 2022 Lesson Outline Clusters File Systems NoSQL Sharding Replication Clusters 3 What is a Cluster? A cluster is a tightly coupled collection of servers Also known as nodes Clustered servers usually: Have the same hardware specifications Are connected via a network Work as a single unit Cluster Hardware Each node in the cluster has its own dedicated resources I.e., memory, a processor, hard drive, etc. Tasks are executed by splitting it into small pieces and distributing the execution of the pieces onto different computers on the cluster File Systems 6 What is a File System? Files and File Systems A file system is the method of storing and organizing data on a storage device E.g., flash drives, DVDs, hard drives, etc. A file is an atomic unit of storage used by the file system to store data File Systems & Operating Systems Provides a logical view of the data stored on the storage device Presented as a tree structure of directories and files Operating systems employ file systems to store and retrieve data on behalf of applications Each operating system provides support for one or more file systems Such as: Distributed File System A file system that can store large files spread across the nodes of a cluster To the client, files appear to be local This is only a logical view Physically the files are distributed throughout the cluster Distributed File System Examples This local view is presented via the distributed file system It enables the files to be accessed from multiple locations Examples: Google File System (GFS) Hadoop Distributed File System (HDFS) NoSQL 12 About NoSQL Not Only SQL A non-relational database that is: Highly scalable Fault-tolerant Designed to store: Semi-structured data Unstructured data About NoSQL (continued) Often provides an API-based query interface that can be called from within an application Also support query languages other than Structured Query Language (SQL) SQL was designed to query structured data stored within a relational database Examples: XQuery: A NoSQL database that is optimized to store XML files SPARQL: A NoSQL database designed to store RDF data There are some NoSQL databases that also provide an SQL-like query interface Sharding 15 What is Sharding? The process of horizontally partitioning A large dataset into a collection of smaller, more manageable datasets Called shards What is Sharding? (continued) Each shard is stored on a separate node Each node is responsible for only the data stored on it Each shard shares the same schema All shards collectively represent the complete dataset Sharding Usage Each shard can independently service reads and writes for the specific subset of data that it is responsible for Depending on the query, data may need to be fetched from both shards Sharding Advantages Sharding Disadvantages Replication 21 What is Replication? The storage of multiple copies of a dataset on multiple nodes Copies are known as replicas Two methods of implementing replication: Primary-Secondary Peer-to-Peer Replication Advantages Replication Disadvantages Primary-Secondary Replication All data is written to a primary node Once saved, the data is replicated to multiple secondary nodes All write requests occur on the primary node Read requests can be fulfilled by any secondary node Primary-Secondary Replication Usage Peer-to-Peer Replication All nodes operate at the same level No primary-secondary relationship between the nodes Nodes known as peers Each node is equally capable of handling reads and writes Peer-to-Peer Replication Usage Thank you! Any questions?"
    },
    "lesson07_BigDataProcessing.pptx": {
        "lesson_number": "lesson07",
        "lesson_name": "BigDataProcessing",
        "course_resources": [
            "https://research.google/pubs/pub62/"
        ],
        "citations": [
            "Big Data Fundamentals: Concepts, Drivers & Techniques, 1st Edition, T. Erl, W. Khattak, P. Buhler. Pearson, 2016."
        ],
        "content": "Lesson 07 Big Data Processing Concepts PROG73010 Introduction to Big Data Fall 2022 Lesson Outline Parallel Data Processing Distributed Data Processing Hadoop Batch Mode Processing Real-Time Mode Processing Big Data Processing The need to process large volumes of data is not new The idea of partitioning large datasets into smaller datasets is also not new In Lesson 05 we discussed the idea of a data warehouse and data marts Big Data Processing (continued) In traditional relational databases, processing is usually centralized To handle the volume and velocity of Big Data, it is usually processed in a parallel and distributed fashion at the location in which it is stored Not all Big Data is batch-processed Parallel Data Processing 5 What is Parallel Processing? The simultaneous execution of multiple sub-tasks that collectively comprise a larger task With the goal of reducing execution time Dividing one large task into multiple smaller tasks that run concurrently E.g.: One task of 60 seconds Three tasks of 20 seconds Achieving Parallel Processing Achieved through a single machine with multiple processors or cores Distributed Processing Closely related to parallel data processing Uses the same \"divide-and-conquer\" principle Achieved through physically separate machines that are networked together as a cluster Parallel vs. Distributed Processing Differences Parallel vs. Distributed Processing When to Use Hadoop 11 What is Hadoop? An open-source framework for large-scale data storage and data processing that is compatible with commodity hardware Currently established as a de facto industry platform for contemporary Big Data solutions What is Hadoop? (continued) Can be used as: An Extract, Transform and Load (ETL) engine An analytics engine for processing large amounts of structured, semi-structured and unstructured data For analysis, Hadoop implements the MapReduce processing framework Processing Workloads 14 What is a Processing Workload? The amount and nature of data that is processed within a certain amount of time Workloads are usually divided into two types: Batch Transactional Bath Processing The processing of data in batches Often imposes delays Results in high-latency responses Typically involve large quantities of data with sequential read or write queries Also known as offline processing Bath Processing Example Queries can be complex and involve multiple joins OLAP systems commonly process workloads in batches Strategic BI and analytics are batch-oriented as they are highly read-intensive tasks involving large volumes of data Transactional Processing An approach where data is processed interactively without delay Results in low-latency responses Transaction workloads involve small amounts of data with random reads and writes Transactional Processing Example OLTP and operational systems, which are generally write-intensive, commonly use transactional processing Transactional workloads comprise random reads/writes that involve fewer joins than business intelligence and reporting workloads Big Data Processing and Clustering We have previous discussed how clustering allows for horizontally scalable storage solutions How does clustering affect data processing? Provides the mechanism to enable distributed data processing with linear scalability They provide an environment for processing as large datasets can be divided into smaller datasets and then processed in parallel Big Data datasets can either be processed in batch mode or real-time mode Clustering Advantages They provide inherent redundancy and fault tolerance Physically separate nodes Allow resilient processing and analysis to occur if a network or node failure occurs Leveraging cloud-host infrastructure services, or ready-made analytical environments as the backbone of a cluster, is sensible due to their elasticity and pay-for-use model of utility-based computing Batch Mode Processing 22 What is Batch Mode Processing? Data is processed offline in batches and the response time could vary from minutes to hours Data must be persisted to the disk before it can be processed Batch mode generally involves processing a range of large datasets, either on their own or joined together Volume and variety Most Big Data processing occurs in batch mode Relatively simple, easy to set up and low in cost compared to real-time mode Strategic BI, predictive and prescriptive analytics and ETL operations are commonly batch-oriented What is MapReduce? A widely used implementation of a batch processing framework Highly scalable and reliable and is based on the principle of divide-and-conquer Provides fault tolerance and redundancy Divides a big problem into a collection of smaller problems that can each be solved quickly MapReduce has roots in both distributed and parallel computing About MapReduce Does not require that the input data conform to any data model Can be used to process schema-less datasets A dataset is broken down into multiple smaller parts, and operations are performed on each part independently and in parallel The results are then summarized to arrive at the answer About MapReduce (continued) MapReduce processing engine generally only supports batch workloads Due to the coordination overhead involved in managing a job The work is not expected to have low latency MapReduce is based on a Google whitepaper published in 2004 MapReduce Data Processing vs. Traditional Data Processing MapReduce processing engine works differently compared to traditional data processing paradigm Traditional data processing requires moving data from the storage node to the processing node that runs the data processing algorithm Works for smaller datasets Why would this be a problem for large datasets? MapReduce Processing MapReduce processing is moved to the nodes that store the data The data processing algorithm executes in parallel on these nodes No need to move the data Saves network bandwidth Results in a large reduction in processing time for large datasets, since processing smaller chunks of data in parallel is faster MapReduce Job A single processing run of the MapReduce processing engine is known as a MapReduce job Each MapReduce job is composed of two sets of tasks: Map task Reduce task Each task consists of multiple stages Map Task Map Stage The dataset file is divided into multiple smaller splits Each split is parsed into its constituent records as a key-value pair The key is usually the ordinal position of the record The value is the actual record The parsed key-value pairs for each split are then sent to a map function or mapper, with one mapper function per split The map function executes user-defined logic Map Task Combine Stage Map tasks and reduce tasks are usually executed on different nodes Data must be moved between mappers and reducers Potential for processing latency An optional combine function (combiner) summarizes a mapper's output before it gets processed by the reducer A reducer that runs individually on each mapper The combiner class is often set to the reducer class Can be separate as well Map Task Partition Stage If more than one reducer is involved, a partitioner divides the output from the mapper or combiner into partitions between reducer instances The number of partitions will equal the number of reducers All records for a particular key are assigned to the same partition Reduce Task Shuffle & Sort Task Shuffling: Output from all partitioners is copied across the network to the nodes running the reduce task Sorting: Group and sort the key-value pairs according to the keys so that the output contains a sorted list of all input keys and their values with the same keys appearing together This merge creates a single key-value pair per group The key is the group key, and the value is the list of all group values Reduce Task Reduce Stage The reducer function is user-defined The reducer will either further summarize its input or will emit the output without making any changes In either case, for each key-value pair that a reducer receives, the list of values stored in the value part of the pair is processed and another key-value pair is written out The number of reducers can be customized Possible to have a MapReduce job without a reducer E.g., Performing filtering Output of a reducer is written as a separate file, one file per reducer MapReduce Example Real-Time Mode Processing 36 Processing in Real-Time Mode Data is processed in-memory as it is captured before being persisted to the disk Response time generally ranges from a sub-second to under a minute Real-time mode addresses the velocity characteristic of Big Data datasets Also called event or stream processing The data either arrives continuously (stream) or at intervals (event) The individual event/stream datum is generally small, but its continuous nature results in very large datasets Speed, Consistency and Volume (SCV) Principle A distributed data processing system can be designed to support only two of the following three requirements: Speed Consistency Volume Which two of the three dimensions to support is dependent upon the system requirements of the analysis environment Which is generally unacceptable to lose? Real-Time Analytics Processing Big Data in real-time generally refers to real-time or near-real-time analytics Data is processed as it arrives at the enterprise boundary without an unreasonable delay Instead of initially persisting the data to the disk, the data is first processed in memory and then persisted to the disk for future use or archival purposes Opposite of batch processing mode, where data is persisted to the disk first and then subsequently processed Real-Time Analytics (continued) Analyzing Big Data in real-time requires the use of in-memory storage devices E.g., In-Memory Data Grids (IMDGs) or In-Memory Databases (IMDBs) Once in memory, the data can then be processed in real-time without incurring any hard-disk I/O latency The real-time processing may involve calculating simple statistics, executing complex algorithms or updating the state of the in-memory data as a result of a change detected in some metric. Real-Time Big Data Processing and MapReduce MapReduce is generally unsuitable for real-time Big Data processing Large overhead associated with MapReduce job creation and coordination MapReduce is intended for the batch-oriented processing of large amounts of data that has been stored to disk MapReduce cannot process data incrementally and can only process complete datasets Therefore, all input data must be available in its entirety before the execution of the data processing job This is at odds with the requirements for real-time data processing as real-time processing involves data that is often incomplete and continuously arriving via a stream Thank you! Any questions?"
    },
    "lesson01_BigDataConcepts.pptx": {
        "lesson_number": "lesson01",
        "lesson_name": "BigDataConcepts",
        "course_resources": [],
        "citations": [
            "Big Data Fundamentals: Concepts, Drivers & Techniques, 1st Edition, T. Erl, W. Khattak, P. Buhler. Pearson, 2016."
        ],
        "content": "Lesson 01 Big Data Concepts PROG73010 Introduction to Big Data Fall 2022 Indigenous Land Acknowledgement To recognize the land is an expression of gratitude and appreciation to those whose territory we reside on, and a way of honouring the Indigenous peoples who have been living and working on the land for thousands of years. Agenda Introduction to Big Data Big Data Concepts Big Data Characteristics Types of Data Introduction to Big Data 4 What is \"Big Data\" Analysis, processing and storage of large collections of data Often from different sources Required when traditional data analysis, processing and storage technologies and techniques are insufficient Requirements of Big Data Combine multiple, unrelated datasets Process large amounts of unstructured data Discover information Quickly Roots of Big Data \"New\" discipline The management and analysis of large datasets is a long-standing problem Advances in technology has changed analytic approaches to large dataset The Analysis of Big Data Big Data analysis is an interdisciplinary endeavor Accumulation of Data How do organizations gather data? Big Data Insights What insights can we gather from Big Data analysis? Big Data Concepts 11 Big Data Concepts Dataset Collections of related data Big Data Concepts Data Analysis The process of examining data to find facts, relationships, patterns, insights or trends What is the goal of data analysis? Big Data Concepts Data Analytics Broader discipline that encompasses data analysis Also includes the complete data lifecycle Collecting Cleansing Organizing Storing Analyzing Governing Big Data Concepts Data Analytics Data Analytics Categories Four general categories of data analytics Distinguished by the results they produce Big Data Concepts Data Analytics Descriptive Analysis Answer questions about events that have already occurred Examples? Big Data Concepts Data Analytics Diagnostic Analytics Determine the cause of a past event Examples? Big Data Concepts Data Analytics Predictive Analytics Determine the outcome of an event that may occur in the future Examples? Big Data Concepts Data Analytics Prescriptive Analytics Determine actions that should be taken in the future Examples? Big Data Concepts Business Intelligence An organization gaining insight by analyzing data generated by its business processes and information systems Results used to steer the business Big Data Concepts KPI Key Performance Indicators A metric used to gauge success within a business context Big Data Characteristics 22 Big Data vs. Traditional Data A Big Dataset must possess one or more characteristics that require accommodation in the solution design and architecture of the analytic environment The Five Vs Volume Velocity Variety Veracity Value Big Data Characteristics Volume The size of Big Data is substantial and ever-growing How much data is produced every day? Big Data Characteristics Velocity Big Data arrives at fast speeds and enormous datasets can accumulate in a short amount of time How many tweets are sent each day? How many WhatsApp messages are sent each day? Big Data Characteristics Variety Big Data solutions must support multiple formats and types of data What are some examples? Big Data Characteristics Veracity Big Data strives to have high veracity High signal-to-noise ratio Signal Data that can lead to information Noise Data that cannot be converted into information What are some examples? Big Data Characteristics Value Big Data should be useful to an enterprise What are other V is value closely related with? What else impacts value? Types of Data 29 Source of Data Data can be generated by humans or machines Data Type Categories Types of data used in Big Data solutions: Structured Semi-Structured Unstructured What are some examples of each? Big Data processing, storage and analysis relies on Metadata For what types? Thank you! Any questions?"
    },
    "lesson05_BigDataIntelligence.pptx": {
        "lesson_number": "lesson05",
        "lesson_name": "BigDataIntelligence",
        "course_resources": [],
        "citations": [
            "Big Data Fundamentals: Concepts, Drivers & Techniques, 1st Edition, T. Erl, W. Khattak, P. Buhler. Pearson, 2016."
        ],
        "content": "Lesson 05 Big Data Business Intelligence PROG73010 Introduction to Big Data Fall 2022 Lesson Outline Enterprise Technologies for Data Transformation Online Transaction Processing Online Analytical Processing Extract Transform Load Enterprise Storage Technologies Data warehouse & data marts Business Intelligence Traditional & Big Data Recap: Connecting Business Architecture & Big Data Enterprise Technologies for Data Transformation 4 Online Transaction Processing (OLTP) A software system that processes transaction-oriented data Online transaction refers to the completion of an activity in real-time Not batch-processed OLTP systems store operational data that is normalized OLTP Uses This data is a common source of structured data and serves as input to many analytic processes The queries supported by OLTP systems are comprised of simple insert, delete and update operations Big Data analysis results can be used to augment OLTP data stored in the underlying relational databases OLTP systems execute business processes in support of corporate operations E.g., a point-of-sale system, ticket reservation system, etc. Online Analytical Processing (OLAP) OLAP systems are used for processing data analysis queries OLAPs form an integral part of business intelligence, data mining and machine learning processes Relevant to Big Data in that they can serve as both a data source as well as a data sink that can receiving data Used in diagnostic, predictive and prescriptive analytics Extract Transform Load (ETL) A process of loading data from a source system into a target system The source system can be a database, a flat file, or an application The target system can be a database or some other storage system ELT Uses ETL represents the main operation through which data warehouses are fed data A Big Data solution encompasses the ETL feature-set for converting data of different types Enterprise Storage Technologies 10 Data Warehouse A central, enterprise-wide repository consisting of historical and current data Data warehouses are heavily used by BI to run various analytical queries Often interface with an OLAP system to support multi-dimensional analytical queries Data Warehouse Problems Data pertaining to multiple business entities from different operational systems is periodically extracted, validated, transformed and consolidated into a single denormalized database With periodic data imports from across the enterprise, the amount of data contained in each data warehouse will continue to increase Over time this leads to slower query response times for data analysis tasks Data Warehouse Solutions To resolve this shortcoming, data warehouses usually contain optimized databases, called analytical databases, to handle reporting and data analysis tasks An analytical database can exist as a separate DBMS, as in the case of an OLAP database. Data Mart A subset of the data stored in a data warehouse Data warehouses can have multiple data marts Typically belongs to a specific part of an organization A department, division, etc. Business Intelligence 15 Traditional Business Intelligence Primarily utilizes descriptive and diagnostic analytics to provide information on historical and current events Provides answers to correctly formulated questions It is not \"intelligent\" Reports on different KPIs through: Ad-hoc reports Dashboards Traditional Business Intelligence Ad-hoc Reports Ad-hoc reporting is a process that involves manually processing data to produce custom-made reports The focus of an ad-hoc report is usually on a specific area of the business E.g., marketing, supply chain management, etc. Custom reports are detailed and often tabular in nature Traditional Business Intelligence Dashboards Provide a holistic view of key business areas Information displayed is generated at periodic intervals in real-time or near-real-time Data is presented using graphics Bar charts, pie charts, gauges, etc. Big Data Business Intelligence Builds upon traditional BI Acts on the cleansed, consolidated enterprise-wide data in the data warehouse and combining it with semi-structured and unstructured data sources Comprises both predictive and prescriptive analytics to facilitate the development of an enterprise-wide understanding of business performance Big Data Business Intelligence Focus Traditional BI analyses generally focus on individual business processes Big Data BI analyses focus on multiple business processes simultaneously Reveals patterns and anomalies across a broader scope within the enterprise Leads to data discovery by identifying insights and information that may have been previously absent or unknown Big Data Business Intelligence Requirements Big Data BI requires the analysis of unstructured, semi-structured and structured data residing in the enterprise data warehouse This requires a non-traditional data warehouse Uses new features and technologies to store cleansed data originating from a variety of sources in a single uniform data format Couples a traditional data warehouse with new technologies results in a hybrid data warehouse Big Data Business Intelligence Hybrid Warehouse Acts as a uniform and central repository of structured, semi-structured and unstructured data Can provide Big Data BI tools with their required data Eliminates the need for Big Data BI tools to have to connect to multiple data sources to retrieve or access data Data Visualization for Big Data Requires data visualization tools that can: Connect to structured, semi-structured and unstructured data sources Capable of handling millions of data records Generally, use in-memory analytical technologies that reduce the latency normally attributed to traditional, disk-based data visualization tools Advanced Data Visualization for Big Data Advanced data visualization tools for Big Data solutions incorporate predictive and prescriptive data analytics and data transformation features These tools eliminate the need for data pre-processing methods, such as ETL Advanced data visualization tools can join structured and unstructured data that is kept in memory For fast data access Queries and statistical formulas can then be applied Common Big Data Visualization Tools Aggregation provides a holistic and summarized view of data across multiple contexts Drill-down enables a detailed view of the data of interest by focusing in on a data subset from the summarized view Filtering helps focus on a particular set of data by filtering away the data that is not of immediate interest Roll-up groups data across multiple categories to show subtotals and totals What-if analysis enables multiple outcomes to be visualized by enabling related factors to be dynamically changed Thank you! Any questions?"
    },
    "lesson04_MoreConsiderations.pptx": {
        "lesson_number": "lesson04",
        "lesson_name": "MoreConsiderations",
        "course_resources": [],
        "citations": [
            "Big Data Fundamentals: Concepts, Drivers & Techniques, 1st Edition, T. Erl, W. Khattak, P. Buhler. Pearson, 2016."
        ],
        "content": "Lesson 04 Adoption Considerations PROG73010 Introduction to Big Data Fall 2022 Lesson Outline Big Data Privacy Big Data Security Provenance Maintaining Big Data Solutions Big Data Privacy 3 What is Privacy? Not security, but often associated with security Security & Privacy Unlike other security related topics, privacy is subjective and difficult to define Authentication Authorization Auditing What is Privacy? (continued) Often used interchangeably with confidentiality Privacy Definition The ability to protect information about oneself that has not been released, as well as the ability to retain some level of control over information that has been released Private Information Private Information Information that describes an individual in some way How much it describes varies Personally Identifiable Information (PII) Big Data Privacy Concerns Performing analytics on datasets can reveal confidential information about organizations or individuals Even analyzing separate datasets that contain seemingly benign data can reveal private information when the datasets are analyzed jointly This can lead to intentional or inadvertent breaches of privacy Addressing Privacy Concerns Addressing these privacy concerns requires an understanding of: The nature of data being accumulated Relevant data privacy regulations Techniques for data tagging and anonymization Examples? Big Data Privacy Concern Examples Big Data Security 10 Big Data Security Concerns Some components of Big Data solutions lack the robustness of traditional enterprise solution environments when it comes to access control and data security Securing Big Data involves ensuring that the data networks and repositories are sufficiently secured Authentication Authorization Data Access Security Concerns Big Data security further involves establishing data access levels for different categories of users Unlike traditional relational database management systems, NoSQL databases generally do not provide robust built-in security mechanisms NoSQL Database Concerns Instead rely on simple HTTP-based APIs where data is exchanged in plaintext This data is more prone to network-based attacks Provenance Information 14 What is Provenance Information? Information about the source of the data How it has been processed What can it be used for? What is Provenance Information Maintaining provenance as large volumes of data are acquired, combined and put through multiple processing stages can be a complex task At different stages in the analytics lifecycle, data will be in different states due to the fact it may be being transmitted, processed or in storage Data-in-motion Data-in-use Data-at-rest Whenever Big Data changes state, it should trigger the capture of provenance information that is recorded as metadata Goal of Provenance Information The goal of capturing provenance is to be able to reason over the generated analytic results: With the knowledge of the origin of the data What steps or algorithms were used to process the data that led to the result If results cannot be justified and repeated, they lack credibility Capturing Provenance Information As data enters the analytic environment, its provenance record can be initialized When provenance information is captured on the way to generating analytic results, the results can be more easily trusted and used with confidence Maintaining Big Data Solutions 19 Maintenance of Big Data Solutions Big Data solutions not only access data, but generate data as well All this data becomes an asset of the organization This data and environment needs to be controlled and maintained A governance framework can help control how the data and solution environment are: Regulated Standardized Evolved Governance Frameworks Examples of what a Big Data governance framework can encompass include: Standardization of how data is tagged and the metadata used for tagging Policies that regulate the kind of external data that may be acquired Policies regarding the management of data privacy and data anonymization Policies for the archiving of data sources and analysis results Policies that establish guidelines for data cleansing and filtering Controlling Data Flows A methodology will be required to control how data flows into and out of Big Data solutions It will need to consider how feedback loops can be established to enable the processed data to undergo repeated refinement Clouds Clouds provide remote environments that can host IT infrastructure for large-scale storage and processing The adoption of a Big Data environment may necessitate that some or all of the environment be hosted within a cloud Example? Cloud Justification Common justifications for incorporating a cloud environment in support of a Big Data solution include: Inadequate in-house hardware resources Upfront capital investment for system procurement is not available The project is to be isolated from the rest of the business so that existing business processes are not impacted The Big Data initiative is a proof of concept Datasets that need to be processed are already cloud resident The limits of available computing and storage resources used by an in-house Big Data solution are being reached Big Data Analytics Lifecycle Big Data analysis is unique due to the volume, velocity and variety characteristics of the data being processes To address the distinct requirements for performing analysis on Big Data, a step-by-step methodology can organize the tasks required to: Acquiring data Processing data Analyzing data Repurposing data Big Data Analytics Lifecycle Stages The Big Data analytics lifecycle can be divided into nine stages Big Data Analytics Lifecycle Stages Business Case Evaluation In this stage a business case can be created, assessed and approved prior to proceeding with the actual hands-on analysis tasks An evaluation of a Big Data analytics business case helps decision-makers understand the business resources that will need to be utilized and which business challenges the analysis will tackle Is the business problem being addressed really a Big Data problem? What budget is required to carry out the analysis project? Required purchases can be weighed against the expected benefits of achieving the goals Big Data Analytics Lifecycle Stages Data Identification This stage is dedicated to identifying the datasets required for the analysis project and their sources Identifying a wider variety of data sources may increase the probability of finding hidden patterns and correlations Internal or external? Big Data Analytics Lifecycle Stages Data Acquisition & Filtering In this stage the data is gathered from all identified data sources Acquired data is subjected to automated filtering Removal of noise data Both internal and external data needs to be persisted once it gets generated or enters the enterprise boundary Metadata can be added via automation to data from both internal and external data sources to improve the classification and querying Big Data Analytics Lifecycle Stages Data Extraction This stage is dedicated to extracting disparate data and transforming it into a format that the underlying Big Data solution can use The extent of extraction and transformation required depends on the types of analytics and capabilities of the Big Data solution Big Data Analytics Lifecycle Stages Data Validation & Cleansing This stage is dedicated to establishing often complex validation rules and removing any known invalid data Big Data solutions often receive redundant data across different datasets Redundant data can be exploited to explore interconnected datasets in order to assemble validation parameters and fill in missing valid data For batch analytics, data validation and cleansing can be achieved via an offline operations For real-time analytics, a more complex in-memory system is required to validate and cleanse the data as it arrives from the source Big Data Analytics Lifecycle Stages Data Aggregation & Representation This stage is dedicated to integrating multiple datasets together to arrive at a unified view Performing this stage can become complicated because of differences in: Data Structure: Although the data format may be the same, the data model may be different Semantics: A value that is labeled differently in two different datasets may mean the same thing, for example \"surname\" and \"last name\" The large volumes processed by Big Data solutions can make data aggregation a time and effort-intensive operation Reconciling these differences can require complex logic that is executed automatically without the need for human intervention Big Data Analytics Lifecycle Stages Data Analysis This stage is dedicated to carrying out the actual analysis task May involve one or more types of analytics This stage can be iterative in nature, especially if the data analysis is exploratory In such a case, analysis is repeated until the appropriate pattern or correlation is uncovered Depending on the type of analytic result required, this stage can be as simple or complex Big Data Analytics Lifecycle Stages Data Visualization This stage is dedicated to using data visualization techniques and tools to graphically communicate the analysis results for effective interpretation by business users Business users need to be able to understand the results in order to obtain value from the analysis and subsequently can provide feedback Data Visualization allows users with the ability to perform visual analysis Allowing for the discovery of answers to questions that users have not yet even formulated Big Data Analytics Lifecycle Stages Utilization of Analysis Results This stage is dedicated to determining how and where processed analysis data can be further leveraged Depending on the nature of the analysis problems being addressed, it is possible for the analysis results to produce models that encapsulate new insights and understandings Common areas that are explored during this stage: Input for Enterprise Systems Business Process Optimization Alerts Thank you! Any questions?"
    },
    "lesson03_AdoptionConsiderations.pptx": {
        "lesson_number": "lesson03",
        "lesson_name": "AdoptionConsiderations",
        "course_resources": [
            "https://data.gov/",
            "https://www.forbes.com/sites/bernardmarr/2016/02/12/big-data-35-brilliant-and-free-data-sources-for-2016/",
            "https://www.usdatacorporation.com/",
            "https://open.canada.ca/en/open-data",
            "https://developer.twitter.com/en/products/twitter-api/enterprise",
            "https://www.kaggle.com/",
            "https://data.ontario.ca/",
            "https://www.acxiom.com/",
            "https://data.waterloo.ca/",
            "https://www.iriworldwide.com/en-gb"
        ],
        "citations": [
            "Big Data Fundamentals: Concepts, Drivers & Techniques, 1st Edition, T. Erl, W. Khattak, P. Buhler. Pearson, 2016."
        ],
        "content": "Lesson 03 Big Data Adoption Considerations PROG73010 Introduction to Big Data Fall 2022 Agenda Organization Prerequisites Big Data Sources Big Data Platforms Organizational Prerequisites 3 Problems with the PDCA Cycle Big Data initiatives are typically strategic in nature and should be business-driven Big Data adoption can be transformative but is more often innovative Transformation activities Low-risk endeavors designed to deliver increased efficiency and effectiveness Innovative activities Requires a shift in mindset because it will fundamentally alter the structure of a business either in its products, services or organization Innovation management requires care Too many controlling forces can stifle the initiative and dampen the results Too little oversight result in promises not delivered Preparing for Big Data Solutions Big Data frameworks are not turn-key solutions Organizations must plan carefully before implementing Big Data solutions What preparations must organizations do? Preparing for Big Data Solutions Whiteboard Big Data Sources 7 Big Data Procurement As previously mentioned, the acquisition of Big Data solutions themselves can be economical Open-source platforms Open-source tools Commodity hardware A substantial budget may still be required to obtain external data The Cost of Data External data can be very valuable to organizations The greater the volume and variety of data that can be supplied, the higher the chances are of finding hidden insights from patterns External data sources include: Government data sources Often includes geo-spatial data and may be free Commercial data markets Most commercially relevant data will need to be purchased and may involve the continuation of subscription costs to ensure the delivery of updates to procured datasets Government Sources of Data Examples Forbes Free Data Sources US Government Data.Gov Government of Canada Open Data Government of Ontario Data Catalogue City of Waterloo Open Data Commercial Sources of Data Examples Acxiom IRi Twitter Kaggle US Data Corporation Big Data Platforms 12 Group Work We will now break into groups and investigate a Big Data Platform Each Group will be assigned a technology Research what it is, what it does, and any advantages or disadvantages Apache Hadoop Apache Spark Amazon EMR Azure Data Lake Cloudera Snowflake Thank you! Any questions?"
    },
    "lesson08_MapReduceJobs.pptx": {
        "lesson_number": "lesson08",
        "lesson_name": "MapReduceJobs",
        "course_resources": [
            "https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/Reducer.html",
            "https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/Mapper.html",
            "https://www.ibm.com/topics/mapreduce",
            "https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html",
            "https://docs.oracle.com/javase/7/docs/api/java/util/StringTokenizer.html"
        ],
        "citations": [
            "Big Data Fundamentals: Concepts, Drivers & Techniques, 1st Edition, T. Erl, W. Khattak, P. Buhler. Pearson, 2016."
        ],
        "content": "Lesson 08 MapReduce Jobs PROG73010 Introduction to Big Data Fall 2022 Lesson Outline Recap Hadoop and MapReduce MapReduce Job Example Lesson Note This lesson will be heavy on discussion, note taking and demonstrations Much of the lesson will be marking up and discussing code This discussion is not included in the slides If you miss this lesson, I recommend watching the video on eConestoga Recap Hadoop and MapReduce 4 Recall: What is Hadoop? An open-source framework for large-scale data storage and data processing that is compatible with commodity hardware Currently established as a de facto industry platform for contemporary Big Data solutions Recall: What is Hadoop? (continued) Can be used as: An Extract, Transform and Load (ETL) engine An analytics engine for processing large amounts of structured, semi-structured and unstructured data For analysis, Hadoop implements the MapReduce processing framework Recall: What is MapReduce? A widely used implementation of a batch processing framework Highly scalable and reliable and is based on the principle of divide-and-conquer Provides fault tolerance and redundancy Divides a big problem into a collection of smaller problems that can each be solved quickly MapReduce has roots in both distributed and parallel computing Recall: About MapReduce Does not require that the input data conform to any data model Can be used to process schema-less datasets A dataset is broken down into multiple smaller parts, and operations are performed on each part independently and in parallel The results are then summarized to arrive at the answer Recall: MapReduce Processing MapReduce processing is moved to the nodes that store the data The data processing algorithm executes in parallel on these nodes No need to move the data Saves network bandwidth Results in a large reduction in processing time for large datasets, since processing smaller chunks of data in parallel is faster Recall: MapReduce Job A single processing run of the MapReduce processing engine is known as a MapReduce job Each MapReduce job is composed of two sets of tasks: Map task Reduce task Each task consists of multiple stages MapReduce Example MapReduce jobs can be written in any language Java Hadoop was developed using Java Python C++ Etc. Link to short video from IBM MapReduce Tasks and Stages Example MapReduce Job Example 13 MapReduce Job Structure MapReduce Jobs consist of a user-defined: Map Combine (optional) Reduce When we create a MapReduce job, we must create the mapper and reducer MapReduce Job Example WordCount In Asssignment 03, I asked you to setup an environment and run a MapReduce job Let's investigate this job The WordCount source code is available here MapReduce WordCount Example Job Classes Three Classes: Mapper public static class IntSumReducer extends Reducer<Text,IntWritable,Text,IntWritable> Reducer public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> Driver public static void main(String[] args) throws Exception This example also uses the java class StringTokenizer MapReduce WordCount Example Job Imports import java.io.IOException; import java.util.StringTokenizer; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; MapReduce WordCount Example Job Mapper public class WordCount { public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable>{ private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key, Text value, Context context ) throws IOException, InterruptedException { StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) { word.set(itr.nextToken()); context.write(word, one); } } } MapReduce WordCount Example Job Reducer public static class IntSumReducer extends Reducer<Text,IntWritable,Text,IntWritable> { private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable<IntWritable> values, Context context ) throws IOException, InterruptedException { int sum = 0; for (IntWritable val : values) { sum += val.get(); } result.set(sum); context.write(key, result); } } MapReduce WordCount Example Job Driver public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); Job job = Job.getInstance(conf, \"word count\"); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); } } MapReduce WordCount Example Job Combiner An optional combine function (combiner) summarizes a mapper's output before it gets processed by the reducer A reducer that runs individually on each mapper The combiner class is often set to the reducer class Can be separate as well public static void main(String[] args) throws Exception { ... job.setCombinerClass(IntSumReducer.class); ... } Thank you! Any questions?"
    },
    "lesson02_AdoptionDrivers.pptx": {
        "lesson_number": "lesson02",
        "lesson_name": "AdoptionDrivers",
        "course_resources": [],
        "citations": [
            "Big Data Fundamentals: Concepts, Drivers & Techniques, 1st Edition, T. Erl, W. Khattak, P. Buhler. Pearson, 2016."
        ],
        "content": "Lesson 02 Big Data Adoption Drivers PROG73010 Introduction to Big Data Fall 2022 Agenda Drivers in the Business Marketplace Big Data and Business Architecture Technological Drivers of Big Data Drivers in the Business Marketplace 3 The Quality Revolution Started in Japan by Deming, Juran, and Ishikawa during 1940s In 1950s, Deming introduced statistical quality control to Japanese engineers Deming introduced Shewhart's PDCA cycle to Japanese researchers It illustrate the activity sequence: Setting goals Assigning them to measurable milestones Assessing the progress against the milestones Take action to improve the process in the next cycle Problems with the PDCA Cycle Traditionally, businesses were driven almost exclusively from internal data held by their information systems Companies are learning that this is not sufficient to execute their business models in a dynamic world-wide marketplace Given rise to the need to consume data from the outside to sense directly the factors that influence their profitability Often results in \"Big Data\" datasets. Shift in the Marketplace Over the past 20 years, we have seen significant corrections in the marketplace 2000 dot-com bubble burst 2008 global recession Ongoing impact of COVID-19 In the short-term, companies have looked for transformation projects to improve processes and achieve savings Competitive Marketplace The global marketplace has never been more competitive The global economy has never been more intertwined As the economy recovered from big shocks, many companies have shifted to an innovation model, offering new products and services and delivering increased value propositions to customers Different than more traditional cost-cutting transformations Expanding BI For these reasons, businesses have looked to expand their Business Intelligence beyond internal information Using external information to gain an understanding of their place in the marketplace Allow an organization to move up the analytic value chain from hindsight, to insight to foresight Transition from Hindsight to Foresight Big Data & Business Architecture 10 Business Architecture Business architecture provides a means of expressing the design of the business Helps an organization align its strategic vision with its underlying execution How can Big Data assist with business architecture? Business Architecture (cont.) Many organizations operate as a layered system Top Layer: Strategic, Executives Middle Layer: Tactical, Managers Bottom Layer: Operational, Workers Connecting Business Architecture & Big Data Technological Drivers of Big Data 14 ICT Drivers of Big Data Many developments in Information and Communications Technologies have driven the adoption of Big Data Such as? ICT Drivers of Big Data Examples Please use your annotation tool Data Analytics & Data Science As we have discussed, organizations are looking for new insights to make their operations more efficient and effective A competitive edge The need for techniques and technologies to extract meaningful information has increased Digitization Digital mediums are replacing physical mediums as the default communication and delivery method For better or for worse Technology Affordability Technology capable of storing and processing large quantities of diverse data has become more affordable Big Data solutions are available using open-source software and commodity hardware Technology no longer delivers competitive advantage in many cases Social Media Allows customers to provide feedback in real-time Forced businesses to consider customer feedback on their service and product offerings in their strategic planning As a result, businesses are storing increasing amounts of data on customer interactions within their customer relationship management systems (CRM) Harvesting customer reviews, complaints and praise from social media sites This information feeds Big Data analysis algorithms that surface the voice of the customer Businesses are increasingly interested in incorporating publicly available datasets from social media and other external data sources Hyper-Connected Communities & Devices Cloud Computing Highly scalable, on-demand resources to build Big Data solutions Drastically reduces the up-front investment of Big Data projects Internet of Everything Combines the services provided by smart connected devices of the Internet of Things into meaningful business processes These processes provide unique and differentiating value propositions A platform for innovation enabling the creation of new products and services and new sources of revenue for businesses Big Data is at the heart of IoE Thank you! Any questions?"
    },
    "lesson11_YARN.pptx": {
        "lesson_number": "lesson11",
        "lesson_name": "YARN",
        "course_resources": [
            "https://dev.mysql.com/blog-archive/mysql-terminology-updates/",
            "https://blogs.sap.com/2017/07/19/bridging-two-worlds-integration-of-sap-and-hadoop-ecosystems/",
            "https://www.nytimes.com/2021/04/13/technology/racist-computer-engineering-terms-ietf.html",
            "https://www.wired.com/story/tech-confronts-use-labels-master-slave/",
            "https://issues.apache.org/jira/browse/HADOOP-17170",
            "https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html",
            "https://github.com/github/renaming"
        ],
        "citations": [
            "Big Data Analytics, V. Ankam. Packt Publishing, 2016.",
            "Introduction to Hadoop YARN, D. Yahalom. Infinite Skills. 2015.",
            "Big Data Fundamentals: Concepts, Drivers & Techniques, 1st Edition, T. Erl, W. Khattak, P. Buhler. Pearson, 2016."
        ],
        "content": "Lesson 11 YARN PROG73010 Introduction to Big Data Fall 2022 Terminology in Tech Technology has long been filled with terminology that is not inclusive Link to GitHub \"Renaming the default branch from master\" Link to MySQL Terminology Updates Link to NYT article, \"'Master', 'Slave' and the Fight Over Offensive Terms in Computing\" Link to Wired article, \"Tech Confronts its Use of the Labels 'Master' and 'Slave' Terminology in Hadoop The current version of Hadoop (3.3.4) still uses this terminology in its official documentation of its architecture Link to Hadoop 3.3.4 HDFS Architecture Link to Hadoop issue tracker on this which remains unresolved In this lesson, I will use Main Nodes and Worker Nodes Lesson Outline Hadoop Ecosystem HDFS Recap MapReduce v1 YARN YARN Demo Hadoop Ecosystem Image from SAP article, Bridging two worlds : Integration of SAP and Hadoop Ecosystems Recall: What is Hadoop? An open-source framework for large-scale data storage and data processing that is compatible with commodity hardware Currently established as a de facto industry platform for contemporary Big Data solutions Recall: Distributed File System A file system that can store large files spread across the nodes of a cluster To the client, files appear to be local This is only a logical view Physically the files are distributed throughout the cluster Advantages HDFS provides: Redundant storage Parallel read performance High scalability HDFS Processes HDFS works in a Main-Worker architecture NameNode Process Runs on Main Node Responsible for HDFS metadata File ownership properties, file locations in HDFS, file permissions, names of individual blocks in HDFS Secondary NameNode Process Runs on Main Node HDFS metadata maintenance HDFS Processes (continued) DataNode Process Handles storage of HDFS data with replication HDFS Example Modern HDFS Main Node is a single point of failure Data on Worker Nodes would be fine, but not accessible until Main Node is restored Solution: Deploy a Standby Main Node Standby Main Node can assume responsibilities of the Secondary NameNode MapReduce v1 Components MapReduce v1, or Classic MapReduce, consisted of three main components An API to develop MapReduce-based applications A framework to execute mappers, shuffle the data, and execute reducers A resource management framework to schedule and monitor resources MapReduce v1 Processes JobTracker process One per cluster on the Main Node Manage MapReduce jobs Distribute tasks to TaskTrackers TaskTracker process One per Worker Node Start, execute and monitor MapReduce tasks MapReduce v1 Whiteboard Example MapReduce v1 Challenges Inflexible, fixed sized CPU slots configured on Worker Nodes within a cluster for Map and Reduce tasks led to the underutilization of the cluster Resources could not be shared with non-MapReduce applications E.g., Spark Limited scalability Only up to 4,000 nodes Limited resource management processes One JobTracker per cluster MapReduce v2 MapReduce v2, or NextGen MapReduce, was created to address these limitations Resource management moved to new implementation, YARN YARN Yet Another Resource Manager Introduced in 2012 by Yahoo! and Hortonworks YARN created to divide the two major responsibilities of the MapReduce v1 JobTracker and TaskTracker into separate entities: ResourceManager NodeManager Application Container ApplicationMaster Job History Server (optional) YARN Components ResourceManager One per cluster Tracks the resource availability of the entire cluster Provides resources to applications when requested by ApplicationMaster Schedules resources on the Worker Nodes Controls application startup YARN Components NodeManager One per Worker Node Responsible for launching containers provided by ResourceManager Starts processes for a running application Manages resources on the Worker Nodes Monitors the resource usage on the Worker Nodes and reports to the ResourceManager YARN Components Application Container Allocation of application containers is handled by the ResourceManager Containers are an allocation of Worker Node CPU and memory Responsible for running the tasks of the application YARN Components ApplicationMaster One per application Runs inside an application container Negotiates the resources needed by an application to run their tasks Responsible for requesting additional containers on behalf of the application Tracks and monitors the progress of the application YARN Components Job History Server Optional process, but recommended One per cluster on Main Node Responsible for archiving job log files This process is specific for MapReduce ResourceManager does not store job history YARN Whiteboard Example MapReduce v1 vs. MapReduce v2 (YARN) One per application Runs inside an application container Negotiates the resources needed by an application to run their tasks Responsible for requesting additional containers on behalf of the application Tracks and monitors the progress of the application Hadoop 1.0 vs. Hadoop 2.0 YARN Summary MapReduce v2 is based on YARN YARN replaced the JobTracker and TaskTracker architecture of MapReduce v1 with the ResourceManager and NodeManager The ResourceManager takes care of scheduling and resource allocation The ApplicationMaster schedules tasks in containers and monitors the tasks YARN Summary (continued) Why YARN? Better scalability Efficient resource management Flexibility to run multiple frameworks Views from the user's perspective: No significant changes Same API, CLI, and web UIs Backward-compatible with MapReduce v1 without any changes YARN Demo Hadoop has built-in command line interface for YARN View running, finished or killed YARN applications View YARN log files Kill running YARN applications Thank you! Any questions?"
    },
    "lesson12_Spark.pptx": {
        "lesson_number": "lesson12",
        "lesson_name": "Spark",
        "course_resources": [
            "https://blogs.sap.com/2017/07/19/bridging-two-worlds-integration-of-sap-and-hadoop-ecosystems/"
        ],
        "citations": [
            "Scala and Spark for Big Data Analytics, Md. R. Karim, S. Alla. Packt Publishing, 2017.",
            "Learning Spark, 2nd Edition, J. S. Damji, B. Wenig, T. Das, D. Lee. O'Reilly Media Inc., 2020."
        ],
        "content": "Lesson 12 Spark PROG73010 Introduction to Big Data Fall 2022 Lesson Outline Hadoop Ecosystem Spark Scala Spark Demo Hadoop Ecosystem Image from SAP article, Bridging two worlds : Integration of SAP and Hadoop Ecosystems History of MapReduce and Hadoop Google's desire to index the Internet for searching This desire led to the creation of the Google File System (GFS) and MapReduce GFS provides a fault-tolerant and distributed filesystem across many commodity hardware nodes MapReduce introduced a new parallel programming paradigm, based on functional programming, for large-scale processing of data distributed over GFS GFS was the blueprint for the Hadoop File System (HDFS) History of MapReduce and Hadoop (continued) Hadoop garnered widespread adoption Inspired a large open-source community of contributors and two open sourcebased commercial companies Cloudera and Hortonworks Now merged However, the MapReduce framework on HDFS had a few shortcomings MapReduce and Hadoop Problems Difficult to manage and administer MapReduce API is verbose and requires a lot of boilerplate setup code, with brittle fault tolerance Uses batch processing Large batches of data jobs can result in many pairs of MapReduce tasks Each pair's intermediate computed result is written to the local disk for the subsequent stage of its operation MapReduce and Hadoop Problems (continued) MapReduce works for large-scale jobs jobs for general batch processing But it falls short for combining other workloads such as machine learning, streaming, or interactive SQL-like queries To solve this, many abstractions created Hive, Impala, Pig, Giraph, etc. Each has its own API and configuration Adds to the complexity of Hadoop Spark History Was there a way to make Hadoop and MapReduce simpler and faster? Researchers at UC Berkeley who had previously worked on Hadoop MapReduce took on this challenge in 2009 A project they called Spark Acknowledged that MR was inefficient (or intractable) for interactive or iterative computing jobs and a complex framework to learn So set the goal of making Spark simpler, faster, and easier Spark History (continued) Early published results showed Spark was 10 to 20 times faster than Hadoop MapReduce for certain jobs Orders of magnitude faster today The central thrust of the Spark project was to bring in ideas borrowed from Hadoop MapReduce, but to enhance the system Make it highly fault tolerant and parallel Support in-memory storage for intermediate results between iterative and interactive map and reduce computations Offer easy and composable APIs in multiple languages as a programming model Support other workloads in a unified manner Spark History (continued 2) By 2013 Spark had gained widespread use Some of its original creators donated the Spark project to Apache and formed a company called Databricks Databricks and the community of open-source developers worked to release Apache Spark 1.0 in May 2014 What is Apache Spark? A unified engine designed for large-scale distributed data processing Provides in-memory storage for intermediate computations Making it much faster than Hadoop MapReduce Incorporates libraries with composable APIs For machine learning: MLlib SQL for interactive queries: Spark SQL Stream processing for interacting with real-time data: Spark Streaming Graph processing: GraphX Spark Design Philosopy Centers around four key characteristics: Speed Ease of use Modularity Extensibility Spark Characteristics Speed Its implementation benefits the current price and performance of CPUs and memory Commodity hardware as nodes Its query computations are built as a directed acyclic graph (DAG) The DAG scheduler and query optimizer construct an efficient computational graph that can usually be decomposed into tasks that are executed in parallel across workers on the cluster Its physical execution engine, Tungsten, uses whole-stage code generation to generate compact code for execution All the intermediate results retained in memory and the limiting of disk I/O provides a huge performance boost Spark Characteristics Ease of Use Provides a fundamental abstraction of a simple logical data structure called a Resilient Distributed Dataset (RDD) upon which all other higher-level structured data abstractions are constructed Spark Characteristics Modularity Spark operations can be applied across many types of workloads and expressed in any of the supported programming languages Scala, Java, Python, SQL, and R Spark offers unified libraries with well-documented APIs that include the following modules as core components Spark SQL, Spark Structured Streaming, Spark MLlib, and GraphX No need for distinct engines for disparate workloads, no need to learn separate APIs Spark Characteristics Extensibility Is focused on being a fast, parallel computation engine rather No storage features Can read read data stored in many sources Apache Hadoop, Apache Cassandra, Apache HBase, MongoDB, RDBMSs, etc. Spark Use Cases Processing in parallel large data sets distributed across a cluster Performing ad hoc or interactive queries to explore and visualize data sets Building, training, and evaluating machine learning models using MLlib Implementing end-to-end data pipelines from myriad streams of data Analyzing graph data sets and social networks Batch Processing vs. Real-Time Processing Scala A general-purpose programming language that comes with support of functional programming and a strong static type system \"Scala\" Scalable Language Designed to grow with the demand of its users The source code of Scala is intended to be compiled into Java bytecode, so that the resulting executable code can be run on a Java virtual machine (JVM) Scala (continued) Martin Odersky started the design of Scala 2001, with first public release in 2004 (Java support only) .NET Framework support in June 2004 Popular and widely adopted because it not only supports the object-oriented programming paradigm, but it also embraces functional programming concepts Scala code is concise and easy to read Spark Demo Thank you! Any questions?"
    }
}