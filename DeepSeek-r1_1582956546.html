<!DOCTYPE html>
<html>
    <head>
        <title>AI Investigation : DeepSeek-r1</title>
        <link rel="stylesheet" href="styles/site.css" type="text/css" />
        <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
    </head>

    <body class="theme-default aui-theme-default">
        <div id="page">
            <div id="main" class="aui-page-panel">
                <div id="main-header">
                    <div id="breadcrumb-section">
                        <ol id="breadcrumbs">
                            <li class="first">
                                <span><a href="index.html">AI Investigation</a></span>
                            </li>
                                                    <li>
                                <span><a href="AI-Investigation_934674757.html">AI Investigation</a></span>
                            </li>
                                                    <li>
                                <span><a href="W25-INDEX_1551106050.html">W25-INDEX</a></span>
                            </li>
                                                    <li>
                                <span><a href="Tech-Research_1583087617.html">Tech Research</a></span>
                            </li>
                                                </ol>
                    </div>
                    <h1 id="title-heading" class="pagetitle">
                                                <span id="title-text">
                            AI Investigation : DeepSeek-r1
                        </span>
                    </h1>
                </div>

                <div id="content" class="view">
                    <div class="page-metadata">
                            
        
    
        
    
        
        
            Created by <span class='author'> Eunie</span>, last modified on Jan 29, 2025
                        </div>
                    <div id="main-content" class="wiki-content group">
                    <p><style>[data-colorid=vtgoxk7rtb]{color:#ff5630} html[data-color-mode=dark] [data-colorid=vtgoxk7rtb]{color:#cf2600}[data-colorid=x6jut2zkmu]{color:#ff5630} html[data-color-mode=dark] [data-colorid=x6jut2zkmu]{color:#cf2600}[data-colorid=hoqk7bxgpl]{color:#bf2600} html[data-color-mode=dark] [data-colorid=hoqk7bxgpl]{color:#ff6640}[data-colorid=mpj0zr52xo]{color:#bf2600} html[data-color-mode=dark] [data-colorid=mpj0zr52xo]{color:#ff6640}</style><strong>DeepSeek r1 (Released January 2025)</strong></p><ul><li><p>DeepSeek r1 has gained significant attention for its robust reasoning capabilities and competitive pricing compared to OpenAI&rsquo;s models.</p></li><li><p>DeepSeek-r1 is recognized for its excellent reasoning capabilities. According to evaluation metrics for comparing large language models (LLMs), its performance is reportedly comparable to OpenAI&rsquo;s O1 version.</p></li></ul><hr /><p><strong>[Key Features]</strong></p><p><strong>1. Performance Metrics</strong></p><ul><li><p> Multiple benchmarks have demonstrated DeepSeek r1&rsquo;s outstanding performance.</p></li><li><p>Balances efficiency and capability, making it suitable for various applications.</p></li></ul><p><a class="external-link" data-card-appearance="inline" href="https://www.deepseek.com/" rel="nofollow">https://www.deepseek.com/</a> </p><p><a class="external-link" href="https://dev.to/nodeshiftcloud/a-step-by-step-guide-to-install-deepseek-r1-locally-with-ollama-vllm-or-transformers-44a1" rel="nofollow">Guide to Install DeepSeek-R1</a></p><p><a class="external-link" href="https://dev.to/maximsaplin/deepseek-r1-vs-openai-o1-1ijm" rel="nofollow">DeepSeek r1 vs. OpenAI O1 Comparison</a></p><p><strong>2. Reasoning Capabilities</strong></p><ul><li><p>Reasoning refers to the ability of LLMs to analyze input, draw conclusions, and generate contextually relevant responses, making it an appropriate term for describing inference performance.</p></li><li><p>Excels at understanding and adapting to user inputs, assessing knowledge levels, and generating personalized content or curricula.</p></li><li><p>Demonstrates exceptional performance in reasoning-based tasks, with comparative tests across three question categories showing results equal to or better than other leading models.</p></li><li><p><strong>Test 1: Reasoning Capability Evaluation &rarr;</strong><a href="1583153170.html" data-linked-resource-id="1583153170" data-linked-resource-version="2" data-linked-resource-type="page"><strong> See below for questions and answers</strong></a></p></li></ul><p><strong>3. Local Deployment Efficiency</strong></p><ul><li><p>It operates as an&nbsp;<strong>7B parameter model and is</strong> optimized for local environments using <strong>OLLAMA</strong> (Optimized for Local Large Model Applications).</p></li><li><p><strong>Model size</strong>: <strong>4.9GB</strong></p></li><li><p><strong>Maximum VRAM requirement</strong>: <strong>16GB</strong></p></li><li><p>Designed for local deployment with minimal hardware requirements, offering a balance of <strong>speed</strong> and <strong>cost efficiency.</strong></p></li><li><p><a class="external-link" href="https://dev.to/nodeshiftcloud/a-step-by-step-guide-to-install-deepseek-r1-locally-with-ollama-vllm-or-transformers-44a1" rel="nofollow">Guide to Install DeepSeek-R1</a></p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="Prerequisites-20250128-060350.png" width="736" loading="lazy" src="attachments/1582956546/1583087657.png?width=736" data-image-src="attachments/1582956546/1583087657.png" data-height="681" data-width="822" data-unresolved-comment-count="0" data-linked-resource-id="1583087657" data-linked-resource-version="2" data-linked-resource-type="attachment" data-linked-resource-default-alias="Prerequisites-20250128-060350.png" data-base-url="https://varlab-dev.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="1582956546" data-linked-resource-container-version="10" data-media-id="f1910a8d-4d37-4cdb-80d9-de68c5a986f7" data-media-type="file" /></span></li><li><p><strong>Comparison with Other Models</strong></p><ul><li><p><strong>Phi-4</strong></p><ul><li><p><strong>Model size</strong>: <strong>9.1GB</strong></p></li><li><p>It requires&nbsp;48GB of&nbsp;<strong>VRAM</strong>, making it less suitable for local deployment than DeepSeek r1.</p></li></ul></li><li><p><strong>Mistral</strong></p><ul><li><p> <strong>Model size</strong>: <strong>26GB</strong></p></li><li><p>Demands significantly higher hardware resources, making it challenging to use locally without high-end setups.</p></li></ul></li></ul></li><li><p><strong>Test 2: Local Performance and Accessibility </strong></p><ul><li><p><strong><span data-colorid="x6jut2zkmu">Performance Analysis: Token Evaluation and Processing: </span></strong></p><ul><li><p>An appealing aspect is that the LLM model can operate efficiently on low-spec local hardware.</p></li></ul></li></ul></li></ul><div class="table-wrap"><table data-table-width="760" data-layout="default" data-local-id="5a94f921-a563-4b3c-b623-905bc1e85c04" class="confluenceTable"><colgroup><col style="width: 353.0px;" /><col style="width: 405.0px;" /></colgroup><tbody><tr><th class="confluenceTh"><p style="text-align: center;"><strong>Hardware Environment</strong></p></th><th class="confluenceTh"><p style="text-align: center;"><strong>Deepseek-r1:7b Performance</strong></p></th></tr><tr><td class="confluenceTd"><p><strong>[ Azure VM ]</strong></p><ul><li><p>OS: Linux</p></li><li><p>CPU: Intel&reg; Xeon&reg; Platinum 8171M @ 2.60 GHz (2 cores, 2 threads)</p></li><li><p>RAM: 7.7 GB (available: 6.8 GB)</p></li><li><p>GPU: None</p></li></ul></td><td class="confluenceTd"><p>total duration:       2m8.364592469s<br />load duration:        27.043374ms<br />prompt eval count:    32 token(s)<br />prompt eval duration: 2.841s<br />prompt eval rate:     11.26 tokens/s<br />eval count:           268 token(s)<br />eval duration:        2m4.446s<br /><span data-colorid="mpj0zr52xo">eval rate:            2.15 tokens/s</span></p><p /><p>[Reference: 1.5b model performance]</p><p>total duration:       54.342693777s<br />load duration:        25.746617ms<br />prompt eval count:    30 token(s)<br />prompt eval duration: 663ms<br />prompt eval rate:     45.25 tokens/s<br />eval count:           473 token(s)<br />eval duration:        52.735s<br />eval rate:           <span data-colorid="hoqk7bxgpl"> 8.97 tokens/s</span></p></td></tr><tr><td class="confluenceTd"><p><strong>[Labtop]</strong></p><ul><li><p><strong>OS</strong>: Windows 11</p></li><li><p><strong>CPU</strong>: 11th Gen Intel&reg; Core&trade; i7-11800H @ 2.30 GHz (8 cores, 16 threads)</p></li><li><p><strong>RAM</strong>: 16 GB</p></li><li><p><strong>GPU</strong>:</p><ul><li><p>NVIDIA RTX A3000 (VRAM: 4 GB)</p></li><li><p>Intel&reg; UHD Graphics (VRAM: 2 GB)</p></li></ul></li></ul></td><td class="confluenceTd"><p>total duration:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2m6.0475139s<br />load duration:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.4502375s<br />prompt eval count:&nbsp;&nbsp;&nbsp; 1078 token(s)<br />prompt eval duration: 1.436s<br />prompt eval rate:&nbsp;&nbsp;&nbsp;&nbsp; 750.70 tokens/s<br />eval count:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1717 token(s)<br />eval duration:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1m59.322s<br /><span data-colorid="vtgoxk7rtb">eval rate:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 14.39 tokens/s</span></p></td></tr></tbody></table></div><p /><hr /><p><strong>[Deployment Considerations]</strong></p><p><strong>1. Memory Management</strong></p><p>&bull; Efficient management of GPU/CPU resources is crucial for smooth local deployment and avoiding performance bottlenecks.</p><p><strong>2. Cloud Deployment</strong></p><p>&bull; In cases of hardware issues, cloud-based APIs provide an alternative deployment method. However, additional costs are incurred, and latency may increase compared to local deployment.</p><p /><hr /><p><strong>[Conclusion]</strong></p><ul><li><p>The lightweight DeepSeek-R1 model appears to deliver excellent results with fast performance, even on low-spec local systems.</p><ul><li><p>However, its performance may vary depending on the prompt type. For example, while the lightweight local model performs well in zero-shot scenarios, it may experience performance degradation in few-shot prompting.</p></li><li><p>Therefore, it is necessary to adjust the prompts and evaluate actual performance. </p></li><li><p>Reference : <a class="external-link" data-card-appearance="inline" href="https://medium.com/data-science-in-your-pocket/deepseek-r1-distill-qwen-1-5b-the-best-small-sized-llm-14eee304d94b" rel="nofollow">https://medium.com/data-science-in-your-pocket/deepseek-r1-distill-qwen-1-5b-the-best-small-sized-llm-14eee304d94b</a><br /> </p></li></ul></li></ul><ul><li><p>After discussions with the team, we have decided to proceed with both <strong>Mistral and DeepSeek-R1</strong> for upcoming tasks. This approach will allow us to evaluate and determine which model is best suited for our specific use cases as the project progresses.</p></li></ul><p /><p />
                    </div>

                                        <div class="pageSection group">
                        <div class="pageSectionHeader">
                            <h2 id="attachments" class="pageSectionTitle">Attachments:</h2>
                        </div>

                        <div class="greybox" align="left">
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/1582956546/1583316996.png">Prerequisites-20250128-060350.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/1582956546/1583087657.png">Prerequisites-20250128-060350.png</a> (image/png)
                                <br/>
                                                    </div>
                    </div>
                    
                                                      
                </div>             </div> 
            <div id="footer" role="contentinfo">
                <section class="footer-body">
                    <p>Document generated by Confluence on May 16, 2025 14:27</p>
                    <div id="footer-logo"><a href="http://www.atlassian.com/">Atlassian</a></div>
                </section>
            </div>
        </div>     </body>
</html>
