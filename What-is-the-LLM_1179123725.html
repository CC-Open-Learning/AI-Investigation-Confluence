<!DOCTYPE html>
<html>
    <head>
        <title>AI Investigation : What is the LLM</title>
        <link rel="stylesheet" href="styles/site.css" type="text/css" />
        <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
    </head>

    <body class="theme-default aui-theme-default">
        <div id="page">
            <div id="main" class="aui-page-panel">
                <div id="main-header">
                    <div id="breadcrumb-section">
                        <ol id="breadcrumbs">
                            <li class="first">
                                <span><a href="index.html">AI Investigation</a></span>
                            </li>
                                                    <li>
                                <span><a href="AI-Investigation_934674757.html">AI Investigation</a></span>
                            </li>
                                                    <li>
                                <span><a href="S24-INDEX_1249640453.html">S24-INDEX</a></span>
                            </li>
                                                    <li>
                                <span><a href="Tech-Stack_1178992716.html">Tech Stack</a></span>
                            </li>
                                                </ol>
                    </div>
                    <h1 id="title-heading" class="pagetitle">
                                                <span id="title-text">
                            AI Investigation : What is the LLM
                        </span>
                    </h1>
                </div>

                <div id="content" class="view">
                    <div class="page-metadata">
                            
        
    
        
    
        
        
            Created by <span class='author'> Jongeon</span>, last modified on Jun 19, 2024
                        </div>
                    <div id="main-content" class="wiki-content group">
                    <h2 id="WhatistheLLM-WhatistheLLM(LargeLanguageModel)?"><style>[data-colorid=wrucm8sx7n]{color:#ff5630} html[data-color-mode=dark] [data-colorid=wrucm8sx7n]{color:#cf2600}[data-colorid=smkjan8gt2]{color:#4c9aff} html[data-color-mode=dark] [data-colorid=smkjan8gt2]{color:#004eb3}</style>What is the LLM(Large Language Model)?</h2><p>Large Language Models (LLMs) are AI-based language models trained on huge datasets. They are used for Natural Language Processing (NLP) tasks. LLMs can understand the structure, context, and meaning of sentences, allowing them to answer questions, summarize, translate, and generate text.</p><p>These models are based on the transformer neural network architecture and include billions of parameters. They are trained using large amounts of data collected from sources like the internet. The transformer model consists of encoders and decoders with a self-attention mechanism, processing inputs in parallel, which significantly reduces training time. LLMs learn the patterns and structures of human language, allowing for unsupervised learning. This results in excellent performance and capabilities in various NLP tasks.</p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20240618-145041.png" width="702" loading="lazy" src="attachments/1179123725/1179025426.png?width=702" data-image-src="attachments/1179123725/1179025426.png" data-height="865" data-width="1243" data-unresolved-comment-count="0" data-linked-resource-id="1179025426" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20240618-145041.png" data-base-url="https://varlab-dev.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="1179123725" data-linked-resource-container-version="3" data-media-id="76bf4ef2-f741-4d93-aa1d-faf9efa79dc5" data-media-type="file" /></span><p><strong><span data-colorid="smkjan8gt2">Pros</span></strong></p><ul><li><p>High accuracy</p></li><li><p>Various application areas</p></li><li><p>Natural language processing</p></li><li><p>Automation capability</p></li><li><p>Large-scale data learning</p></li><li><p>Continuous learning and improvement</p></li><li><p>Multilingual support</p></li><li><p>Creative content generation</p></li><li><p>User-tailored service provision</p></li><li><p>Fast response and processing speed</p></li></ul><p><strong><span data-colorid="wrucm8sx7n">Cons</span></strong></p><ul><li><p>High computational resource requirement</p></li><li><p>High development cost</p></li><li><p>Data bias issue</p></li><li><p>Privacy protection problem</p></li><li><p>Real-time response limitation</p></li><li><p>Lack of model interpretability</p></li><li><p>Ethical issues (e.g., potential misuse)</p></li><li><p>Dependency on training data quality</p></li></ul><h2 id="WhatistheLLM-TypeofLLMmodels">Type of LLM models</h2><div class="table-wrap"><table data-table-width="760" data-layout="default" data-local-id="7abb99a1-fc9d-45e8-b8ec-7306652a44fc" class="confluenceTable"><colgroup><col style="width: 108.0px;" /><col style="width: 134.0px;" /><col style="width: 515.0px;" /></colgroup><tbody><tr><th class="confluenceTh"><p style="text-align: center;"><strong>Model</strong></p></th><th class="confluenceTh"><p style="text-align: center;"><strong>Company</strong></p></th><th class="confluenceTh"><p style="text-align: center;"><strong>Introduction</strong></p></th></tr><tr><td class="confluenceTd"><p style="text-align: center;">GPT-4</p></td><td class="confluenceTd"><p style="text-align: center;">OpenAI</p></td><td class="confluenceTd"><p>GPT-4 is a large language model developed by OpenAI, which exhibits high performance in natural language processing and generation.</p></td></tr><tr><td class="confluenceTd"><p style="text-align: center;">BERT</p></td><td class="confluenceTd"><p style="text-align: center;">Google</p></td><td class="confluenceTd"><p>BERT has strengths in understanding context and is effective in inferring relationships between sentences.</p></td></tr><tr><td class="confluenceTd"><p style="text-align: center;">T5</p></td><td class="confluenceTd"><p style="text-align: center;">Google</p></td><td class="confluenceTd"><p>T5 is a model that can handle a variety of tasks that convert text to text.</p></td></tr><tr><td class="confluenceTd"><p style="text-align: center;">RoBERTa</p></td><td class="confluenceTd"><p style="text-align: center;">Facebook AI</p></td><td class="confluenceTd"><p>RoBERTa is a variant of BERT that improved performance through more data and training time.</p></td></tr></tbody></table></div><h2 id="WhatistheLLM-HowLLMmodelswork">How LLM models work</h2><p>Large language models (LLMs) typically have three architectural components:<br /><strong>Encoder</strong>: Once the tokenizer converts large amounts of text into numerical values called tokens, the encoder generates a meaningful token embedding that places words with similar meanings close to each other in vector space.<br /><strong>Attention mechanism</strong>: This algorithm is used in LLMs to allow the model to focus on specific parts of the input text for relevant words. This algorithm is not considered separate from the encoder and decoder.<br /><strong>Decoder</strong>: The tokenizer converts the tokens back into words that the user can understand. In this process, the LLM predicts the next word, then the next word, for millions of words. Once the model completes the learning process, it can now perform new tasks such as question answering, language translation, and semantic search.</p><p /><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20240618-141706.png" width="643" loading="lazy" src="attachments/1179123725/1178501131.png?width=643" data-image-src="attachments/1179123725/1178501131.png" data-height="508" data-width="331" data-unresolved-comment-count="0" data-linked-resource-id="1178501131" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20240618-141706.png" data-base-url="https://varlab-dev.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="1179123725" data-linked-resource-container-version="3" data-media-id="2d6027fe-fd77-4218-bb0b-54a17ede816b" data-media-type="file" /></span><p /><h2 id="WhatistheLLM-CorrelationbetweenLLMmodelandChatbot">Correlation between LLM model and Chatbot</h2><p>LLM-based chatbots offer natural conversations and personalized responses, suitable for repetitive questions and simple Q&amp;A. However, they may struggle with complex contexts and can produce inaccurate information or suffer from data bias and security issues. It's crucial to understand these limitations and use them properly.</p><h2 id="WhatistheLLM-OtherusesofLLMchatbot">Other uses of LLM chatbot</h2><p><strong>Automatic translation</strong>: Used to provide automatic translation services between multiple languages.<br /><strong>Document summarization</strong>: Provides a feature that summarizes long documents concisely.<br /><strong>Sentiment analysis</strong>: Used to analyze emotions in text and distinguish between positive, negative, and neutral emotions.<br /><strong>Code generation and debugging</strong>: Helps in writing and correcting programming code.</p><p /><p /><h2 id="WhatistheLLM-APPENDIX">APPENDIX</h2><p><strong>LLM</strong>: LLM(Large Language Model) means a language model trained on large datasets. Thiis model can perform various NLP tasks.</p><p><strong>Transformer</strong>: Transformer is a type on NLP model, especially effective for sequence-to-sequence tasks. The Transformer structure consists of an encoder and a decoder.</p><p><strong>NLP</strong>: The NLP(Natural Language Model) model is a type of AI model designed to understand and process human language. It can perform tasks such as text analysis, translation, and summarization.</p><p><strong>Token</strong>: In LLM (Large Language Model), a &quot;token&quot; is the basic unit of processing text by dividing it into small units. Tokens can generally be divided into units of sentences, words, or characters.</p><p><strong>Tokenization</strong>: The process of breaking text into tokens. For example, the sentence &quot;Hello, how are you?&quot; can be tokenized into tokens such as &quot;Hello&quot;, &quot;,&quot;, &quot;how&quot;, &quot;are&quot;, &quot;you&quot;, &quot;?&quot;.</p><p /><p /><p><a class="external-link" data-card-appearance="inline" href="https://a16z.com/emerging-architectures-for-llm-applications/" rel="nofollow">https://a16z.com/emerging-architectures-for-llm-applications/</a> </p><p><a class="external-link" data-card-appearance="inline" href="https://www.databricks.com/glossary/large-language-models-llm" rel="nofollow">https://www.databricks.com/glossary/large-language-models-llm</a> </p>
                    </div>

                                        <div class="pageSection group">
                        <div class="pageSectionHeader">
                            <h2 id="attachments" class="pageSectionTitle">Attachments:</h2>
                        </div>

                        <div class="greybox" align="left">
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/1179123725/1178730511.png">image-20240618-140822.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/1179123725/1178501131.png">image-20240618-141706.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/1179123725/1179353111.png">10ce77d6-a726-4244-8ff5-10e59274bbb1.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/1179123725/1179025426.png">image-20240618-145041.png</a> (image/png)
                                <br/>
                                                    </div>
                    </div>
                    
                                                      
                </div>             </div> 
            <div id="footer" role="contentinfo">
                <section class="footer-body">
                    <p>Document generated by Confluence on May 16, 2025 14:25</p>
                    <div id="footer-logo"><a href="http://www.atlassian.com/">Atlassian</a></div>
                </section>
            </div>
        </div>     </body>
</html>
